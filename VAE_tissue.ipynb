{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a VAE Model\n",
    "Initially designed for 2D input images.\n",
    "Based on this paper: https://arxiv.org/abs/1807.01349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 64      # initial depth to convolve channels into\n",
    "n_channels = 3  # number of channels (RGB)\n",
    "filt_size = 4   # convolution filter size\n",
    "stride = 2      # stride for conv\n",
    "pad = 1         # padding added for conv\n",
    "\n",
    "class VAE2D(nn.Module):\n",
    "    def __init__(self, img_size, n_latent=300):\n",
    "        \n",
    "        # Model setup\n",
    "        #############\n",
    "        super(VAE2D, self).__init__()\n",
    "        self.n_latent = n_latent\n",
    "        n = math.log2(img_size)\n",
    "        assert n == round(n), 'Image size must be a power of 2'  # restrict image input sizes permitted\n",
    "        assert n >= 3, 'Image size must be at least 8'           # low dimensional data won't work well\n",
    "        n = int(n)\n",
    "\n",
    "        # Encoder - first half of VAE\n",
    "        #############################\n",
    "        self.encoder = nn.Sequential()  \n",
    "        # input: n_channels x img_size x img_size\n",
    "        # ouput: depth x conv_img_size^2\n",
    "        # conv_img_size = (img_size - filt_size + 2 * pad) / stride + 1\n",
    "        self.encoder.add_module('input-conv', nn.Conv2d(n_channels, depth, filt_size, stride, pad,\n",
    "                                                        bias=True))\n",
    "        self.encoder.add_module('input-relu', nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Add conv layer for each power of 2 over 3 (min size)\n",
    "        # Pyramid strategy with batch normalization added\n",
    "        for i in range(n - 3):\n",
    "            # input: depth x conv_img_size^2\n",
    "            # output: o_depth x conv_img_size^2\n",
    "            # i_depth = o_depth of previous layer\n",
    "            i_depth = depth * 2 ** i\n",
    "            o_depth = depth * 2 ** (i + 1)\n",
    "            self.encoder.add_module(f'pyramid_{i_depth}-{o_depth}_conv',\n",
    "                                    nn.Conv2d(i_depth, o_depth, filt_size, stride, pad, bias=True))\n",
    "            self.encoder.add_module(f'pyramid_{o_depth}_batchnorm',\n",
    "                                    nn.BatchNorm2d(o_depth))\n",
    "            self.encoder.add_module(f'pyramid_{o_depth}_relu',\n",
    "                                    nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Latent representation\n",
    "        #######################\n",
    "        # Convolve the encoded image into the latent space, once for mu and once for logvar\n",
    "        max_depth = depth * 2 ** (n - 3)\n",
    "        self.conv_mu = nn.Conv2d(max_depth, n_latent, filt_size)      # return the mean of the latent space \n",
    "        self.conv_logvar = nn.Conv2d(max_depth, n_latent, filt_size)  # return the log variance of the same\n",
    "        \n",
    "        \n",
    "        # Decoder - second half of VAE\n",
    "        ##############################\n",
    "        self.decoder = nn.Sequential()\n",
    "        # input: max_depth x conv_img_size^2 (8 x 8)  TODO double check sizes\n",
    "        # output: n_latent x conv_img_size^2 (8 x 8)\n",
    "        # default stride=1, pad=0 for this layer\n",
    "        self.decoder.add_module('input-conv', nn.ConvTranspose2d(n_latent, max_depth, filt_size, bias=True))\n",
    "        self.decoder.add_module('input-batchnorm', nn.BatchNorm2d(max_depth))\n",
    "        self.decoder.add_module('input-relu', nn.ReLU(inplace=True))\n",
    "    \n",
    "        # Reverse the convolution pyramids used in the encoder\n",
    "        for i in range(n - 3, 0, -1):\n",
    "            i_depth = depth * 2 ** i\n",
    "            o_depth = depth * 2 ** (i - 1)\n",
    "            self.decoder.add_module(f'pyramid_{i_depth}-{o_depth}_conv',\n",
    "                                    nn.ConvTranspose2d(i_depth, o_depth, filt_size, stride, pad, bias=True))\n",
    "            self.decoder.add_module(f'pyramid_{o_depth}_batchnorm',\n",
    "                                    nn.BatchNorm2d(o_depth))\n",
    "            self.decoder.add_module(f'pyramid_{o_depth}_relu', nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Final transposed convolution to return to img_size\n",
    "        # Final activation is tanh instead of relu to allow negative pixel output\n",
    "        self.decoder.add_module('output-conv', nn.ConvTranspose2d(depth, n_channels,\n",
    "                                                                  filt_size, stride, pad, bias=True))\n",
    "        self.decoder.add_module('output-tanh', nn.Tanh())\n",
    "\n",
    "        # Model weights init\n",
    "        ####################\n",
    "        # Randomly initialize the model weights using kaiming method\n",
    "        # Reference: \"Delving deep into rectifiers: Surpassing human-level\n",
    "        # performance on ImageNet classification\" - He, K. et al. (2015)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def encode(self, imgs):\n",
    "        \"\"\"\n",
    "        Encode the images into latent space vectors (mean and log variance representation)\n",
    "        input:  imgs   [batch_size, 3, 256, 256]\n",
    "        output: mu     [batch_size, n_latent, 1, 1]\n",
    "                logvar [batch_size, n_latent, 1, 1]\n",
    "        \"\"\"\n",
    "        output = self.encoder(imgs)\n",
    "        output = output.squeeze(-1).squeeze(-1)\n",
    "        return [self.conv_mu(output), self.conv_logvar(output)]\n",
    "\n",
    "    def generate(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Generates a random latent vector using the trained mean and log variance representation\n",
    "        input:  mu     [batch_size, n_latent, 1, 1]\n",
    "                logvar [batch_size, n_latent, 1, 1]\n",
    "        output: gen    [batch_size, n_latent, 1, 1]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        gen = torch.randn_like(std)\n",
    "        return gen.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, gen):\n",
    "        \"\"\"\n",
    "        Restores an image representation from the generated latent vector\n",
    "        input:  gen      [batch_size, n_latent, 1, 1]\n",
    "        output: gen_imgs [batch_size, 3, 256, 256]\n",
    "        \"\"\"\n",
    "        return self.decoder(gen)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        \"\"\"\n",
    "        Generates reconstituted images from input images based on learned representation\n",
    "        input: imgs     [batch_size, 3, 256, 256]\n",
    "        ouput: gen_imgs [batch_size, 3, 256, 256]\n",
    "               mu       [batch_size, n_latent]\n",
    "               logvar   [batch_size, n_latent]\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(imgs)\n",
    "        gen = self.generate(mu, logvar)\n",
    "        return (self.decode(gen),\n",
    "                mu.squeeze(-1).squeeze(-1),\n",
    "                logvar.squeeze(-1).squeeze(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE2D(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE2D(\n",
       "  (encoder): Sequential(\n",
       "    (input-conv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (input-relu): ReLU(inplace)\n",
       "    (pyramid_64-128_conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_128_batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_128_relu): ReLU(inplace)\n",
       "    (pyramid_128-256_conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_256_batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_256_relu): ReLU(inplace)\n",
       "    (pyramid_256-512_conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_512_batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_512_relu): ReLU(inplace)\n",
       "    (pyramid_512-1024_conv): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_1024_batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_1024_relu): ReLU(inplace)\n",
       "    (pyramid_1024-2048_conv): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_2048_batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_2048_relu): ReLU(inplace)\n",
       "  )\n",
       "  (conv_mu): Conv2d(2048, 300, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv_logvar): Conv2d(2048, 300, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (decoder): Sequential(\n",
       "    (input-conv): ConvTranspose2d(300, 2048, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (input-batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (input-relu): ReLU(inplace)\n",
       "    (pyramid_2048-1024_conv): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_1024_batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_1024_relu): ReLU(inplace)\n",
       "    (pyramid_1024-512_conv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_512_batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_512_relu): ReLU(inplace)\n",
       "    (pyramid_512-256_conv): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_256_batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_256_relu): ReLU(inplace)\n",
       "    (pyramid_256-128_conv): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_128_batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_128_relu): ReLU(inplace)\n",
       "    (pyramid_128-64_conv): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_64_batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_64_relu): ReLU(inplace)\n",
       "    (output-conv): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (output-tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a loss function\n",
    "Must be suitable for anomaly detection by recreation similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE2DLoss(nn.Module):\n",
    "    \"\"\"\n",
    "            This criterion is an implementation of VAELoss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size_average=False, kl_weight=1):\n",
    "        super(VAE2DLoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "    def forward(self, recon_x, x, mu, logvar):\n",
    "        \"\"\"\n",
    "        :param recon_x: generating images. [bsz, C, H, W]\n",
    "        :param x: origin images. [bsz, C, H, W]\n",
    "        :param mu: latent mean. [bsz, z_dim]\n",
    "        :param logvar: latent log variance. [bsz, z_dim]\n",
    "        :return loss, loss_details.\n",
    "            loss: a scalar. negative of elbo\n",
    "            loss_details: {'KL': KL, 'reconst_logp': -reconst_err}\n",
    "        \"\"\"\n",
    "        bsz = x.shape[0]\n",
    "        reconst_err = (x - recon_x).pow(2).reshape(bsz, -1)\n",
    "        reconst_err = 0.5 * torch.sum(reconst_err, dim=-1)\n",
    "\n",
    "        # KL(q || p) = -log_sigma + sigma^2/2 + mu^2/2 - 1/2\n",
    "        KL = (-logvar + logvar.exp() + mu.pow(2) - 1) * 0.5\n",
    "        KL = torch.sum(KL, dim=-1)\n",
    "        if self.size_average:\n",
    "            KL = torch.mean(KL)\n",
    "            reconst_err = torch.mean(reconst_err)\n",
    "        else:\n",
    "            KL = torch.sum(KL)\n",
    "            reconst_err = torch.sum(reconst_err)\n",
    "        loss = reconst_err + self.kl_weight * KL\n",
    "        return loss, {'KL': KL, 'reconst_logp': -reconst_err}\n",
    "\n",
    "    def forward_without_reduce(self, recon_x, x, mu, logvar):\n",
    "        \"\"\"\n",
    "        This also compute the vae loss but it's without take mean or take sum\n",
    "        :param recon_x: generating images. [bsz, C, H, W]\n",
    "        :param x: origin images. [bsz, C, H, W]\n",
    "        :param mu: latent mean. [bsz, z_dim]\n",
    "        :param logvar: latent log variance. [bsz, z_dim]\n",
    "        :return: losses. [bsz] and loss details\n",
    "        \"\"\"\n",
    "        bsz = x.shape[0]\n",
    "        reconst_err = (x - recon_x).pow(2).reshape(bsz, -1)\n",
    "        reconst_err = 0.5 * torch.sum(reconst_err, dim=-1)\n",
    "\n",
    "        # KL(q || p) = -log_sigma + sigma^2/2 + mu^2/2 - 1/2\n",
    "        KL = (-logvar + logvar.exp() + mu.pow(2) - 1) * 0.5\n",
    "        KL = torch.sum(KL, dim=-1)\n",
    "\n",
    "        # [bsz]\n",
    "        losses = reconst_err + self.kl_weight * KL\n",
    "        return losses, {'KL': KL, 'reconst_logp': -reconst_err}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val*n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def trainVAE(train_loader, model, criterion, optimizer, epoch, args):\n",
    "    \"\"\"\n",
    "    Iterate through the train data and perform optimization\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    loss_avg = AverageMeter()\n",
    "    kl_avg = AverageMeter()\n",
    "    reconst_logp_avg = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, _) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if args.cuda:\n",
    "            input = input.cuda()\n",
    "\n",
    "        recon_batch, mu, logvar = model(input)\n",
    "        loss, loss_details = criterion(recon_batch, input, mu, logvar)\n",
    "\n",
    "        # record loss\n",
    "        loss_avg.update(loss.item(), input.size(0))\n",
    "        kl_avg.update(loss_details['KL'].item(), input.size(0))\n",
    "        reconst_logp_avg.update(loss_details['reconst_logp'].item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'reconst_logp {reconst_logp_avg.val:.4f} ({reconst_logp_avg.avg:.4f})\\t'\n",
    "                  'kl {kl_avg.val:.4f} ({kl_avg.avg:.4f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, reconst_logp_avg=reconst_logp_avg, kl_avg=kl_avg,\n",
    "                   loss=loss_avg))\n",
    "\n",
    "    return loss_avg.avg, kl_avg.avg, reconst_logp_avg.avg\n",
    "\n",
    "\n",
    "def validateVAE(val_loader, model, criterion, args):\n",
    "    \"\"\"\n",
    "    iterate through the validate set and output the accuracy\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    loss_avg = AverageMeter()\n",
    "    kl_avg = AverageMeter()\n",
    "    reconst_logp_avg = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, _) in enumerate(val_loader):\n",
    "        if args.cuda:\n",
    "            input = input.cuda()\n",
    "\n",
    "        # compute output\n",
    "        recon_batch, mu, logvar = model(input)\n",
    "        loss, loss_details = criterion(recon_batch, input, mu, logvar)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        loss_avg.update(loss.item(), input.size(0))\n",
    "        kl_avg.update(loss_details['KL'].item(), input.size(0))\n",
    "        reconst_logp_avg.update(loss_details['reconst_logp'].item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'reconst_logp {reconst_logp_avg.val:.4f} ({reconst_logp_avg.avg:.4f})\\t'\n",
    "                  'kl {kl_avg.val:.4f} ({kl_avg.avg:.4f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, reconst_logp_avg=reconst_logp_avg,\n",
    "                   kl_avg=kl_avg, loss=loss_avg))\n",
    "    return loss_avg.avg, kl_avg.avg, reconst_logp_avg.avg\n",
    "\n",
    "def evaluateVAE(test_loader, model, criterion, args):\n",
    "    \"\"\"\n",
    "    iterate through test loader and find out average loss of normal and\n",
    "    abnormal\n",
    "    \"\"\"\n",
    "    avg_abnormal_loss = AverageMeter()\n",
    "    avg_normal_loss = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for i, (input, target) in tqdm(enumerate(test_loader)):\n",
    "       if args.cuda:\n",
    "           input = input.cuda()\n",
    "\n",
    "       # compute output\n",
    "       recon_batch, mu, logvar = model(input)\n",
    "       loss, loss_details = criterion(recon_batch, input, mu, logvar)\n",
    "\n",
    "       # if normal\n",
    "       if target.item() == 1:\n",
    "           avg_normal_loss.update(loss.item(), input.size(0))\n",
    "       else:\n",
    "           avg_abnormal_loss.update(loss.item(), input.size(0))\n",
    "\n",
    "    return avg_normal_loss.avg, avg_abnormal_loss.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(img_size, data_path):\n",
    "    \"\"\"\n",
    "    Load the image datasets from vae_train and vae_test\n",
    "    Transform to correct image size\n",
    "    \"\"\"\n",
    "    \n",
    "    train_path = data_path / 'vae_train/train/'\n",
    "    val_path = data_path / 'vae_train/val/'\n",
    "    test_path = data_path / 'vae_test/'\n",
    "    \n",
    "    norm_args = {'mean': [0.5] * n_channels,\n",
    "                 'std': [0.5] * n_channels}\n",
    "    jitter_args = {'brightness': 0.1,\n",
    "                   'contrast': 0.1\n",
    "                   'saturation': 0.1}  # hue unchanged\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomCrop(img_size), # var in horizontal position\n",
    "        transforms.RandomHorizontalFlip(p=0.25),  # var in photo orientation\n",
    "        transforms.RandomVerticalFlip(p=0.25),\n",
    "        transforms.ColorJitter(**jitter_args),  # var in photo lighting\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**norm_args)])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.CenterCrop(img_size) # assume center is most important\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**norm_args)])\n",
    "\n",
    "    train_ds = datasets.ImageFolder(train_path, train_transform)\n",
    "    val_ds = datasets.ImageFolder(val_path, test_transform)\n",
    "    test_ds = datasets.ImageFolder(test_path, test_transform)\n",
    "    \n",
    "    \n",
    "    loader_args = {\n",
    "                   'shuffle': True,\n",
    "                   'num_workers': 4}\n",
    "    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, **loader_args)\n",
    "    val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, **loader_args)\n",
    "    test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=1, ** loader_args)\n",
    "    \n",
    "    return train_dl, val_dl, test_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model/Data\n",
    "desc = 'VAE for detecting anomalies in 2D images'\n",
    "data_path = Path('data/NV_outlier/')\n",
    "img_size = 128\n",
    "n_channels = 3\n",
    "\n",
    "# Training\n",
    "epochs = 40\n",
    "lr = 1e-4                # learning rate\n",
    "lr_decay = 0.1           # lr decay factor\n",
    "kl = 0.01                # weight of the kl term\n",
    "schedule = [10, 20, 30]  # decrease lr at these epochs\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Checkpoints/Logging\n",
    "save_path = Path('model/')  # where model and results will be saved\n",
    "load_path = None            # checkpoint to resume from (default None)\n",
    "log_freq = 10               # print status after this many batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = load_datasets(img_size, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dl.shape, val_dl.shape, test_dl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = VAE2D(img_size)\n",
    "\n",
    "# Load optimizer and scheduler\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optim, schedule, lr_decay)\n",
    "\n",
    "# Load checkpoint if any\n",
    "if load_path is not None:\n",
    "    checkpoint = torch.load(load_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optim.load_state_dict(checkpoint['optimizer'])\n",
    "    print(\"Checkpoint loaded\")\n",
    "    print(f\"Val loss: {checkpoint['val_loss']}\\tEpoch: {checkpoint['epoch']}\")\n",
    "\n",
    "# Set loss criterion\n",
    "criterion = VAE2DLoss(size_average=True, kl_weight=kl)\n",
    "\n",
    "# Move to GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO replace with Visdom - from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make save directory\n",
    "if save_path.is_dir():\n",
    "    print(f\"{save_path} already exists\")\n",
    "else:\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "# TODO - continue\n",
    "    \n",
    "# save args\n",
    "args_dict = vars(args)\n",
    "with open(os.path.join(args.out_dir, 'config.txt'), 'w') as f:\n",
    "    for k in args_dict.keys():\n",
    "        f.write(\"{}:{}\\n\".format(k, args_dict[k]))\n",
    "writer = SummaryWriter(log_dir=os.path.join(args.out_dir, 'logs'))\n",
    "\n",
    "# main loop\n",
    "best_loss = np.inf\n",
    "for epoch in range(args.epochs):\n",
    "    # train for one epoch\n",
    "    scheduler.step()\n",
    "    train_loss, train_kl, train_reconst_logp = trainVAE(train_loader, model, criterion, opt, epoch, args)\n",
    "    writer.add_scalar('train_elbo', -train_loss, global_step=epoch + 1)\n",
    "    writer.add_scalar('train_kl', train_kl, global_step=epoch + 1)\n",
    "    writer.add_scalar('train_reconst_logp', train_reconst_logp, global_step=epoch + 1)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_kl, val_reconst_logp = validateVAE(val_loader, model, criterion, args)\n",
    "        writer.add_scalar('val_elbo', -val_loss, global_step=epoch + 1)\n",
    "        writer.add_scalar('val_kl', val_kl, global_step=epoch + 1)\n",
    "        writer.add_scalar('val_reconst_logp', val_reconst_logp, global_step=epoch + 1)\n",
    "\n",
    "    # remember best acc and save checkpoint\n",
    "    if val_loss < best_loss:\n",
    "        print('checkpointed!')\n",
    "        best_loss = val_loss\n",
    "        save_dict = {'epoch': epoch + 1,\n",
    "                     'state_dict': model.state_dict(),\n",
    "                     'val_loss': val_loss,\n",
    "                     'optimizer': opt.state_dict()}\n",
    "        save_path = os.path.join(args.out_dir, 'best_model.pth.tar')\n",
    "        torch.save(save_dict, save_path)\n",
    "    print('curr lowest val loss {}'.format(best_loss))\n",
    "\n",
    "    # visualize reconst and free sample\n",
    "    print(\"plotting imgs...\")\n",
    "    with torch.no_grad():\n",
    "        val_iter = val_loader.__iter__()\n",
    "\n",
    "        # reconstruct 25 imgs\n",
    "        imgs = val_iter._get_batch()[1][0][:25]\n",
    "        if args.cuda:\n",
    "            imgs = imgs.cuda()\n",
    "        imgs_reconst, mu, logvar = model(imgs)\n",
    "\n",
    "        # sample 25 imgs\n",
    "        noises = torch.randn(25, model.nz, 1, 1)\n",
    "        if args.cuda:\n",
    "            noises = noises.cuda()\n",
    "        samples = model.decode(noises)\n",
    "\n",
    "        def write_image(tag, images):\n",
    "            \"\"\"\n",
    "            write the resulting imgs to tensorboard.\n",
    "            :param tag: The tag for tensorboard\n",
    "            :param images: the torch tensor with range (-1, 1). [9, 3, 256, 256]\n",
    "            \"\"\"\n",
    "            # make it from 0 to 255\n",
    "            images = (images + 1) / 2\n",
    "            grid = make_grid(images, nrow=5, padding=20)\n",
    "            writer.add_image(tag, grid.detach(), global_step=epoch + 1)\n",
    "\n",
    "        write_image(\"origin\", imgs)\n",
    "        write_image(\"reconst\", imgs_reconst)\n",
    "        write_image(\"samples\", samples)\n",
    "        print('done')\n",
    "\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val*n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def trainVAE(train_loader, model, criterion, optimizer, epoch, args):\n",
    "    \"\"\"\n",
    "    Iterate through the train data and perform optimization\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    loss_avg = AverageMeter()\n",
    "    kl_avg = AverageMeter()\n",
    "    reconst_logp_avg = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, _) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if args.cuda:\n",
    "            input = input.cuda()\n",
    "\n",
    "        recon_batch, mu, logvar = model(input)\n",
    "        loss, loss_details = criterion(recon_batch, input, mu, logvar)\n",
    "\n",
    "        # record loss\n",
    "        loss_avg.update(loss.item(), input.size(0))\n",
    "        kl_avg.update(loss_details['KL'].item(), input.size(0))\n",
    "        reconst_logp_avg.update(loss_details['reconst_logp'].item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'reconst_logp {reconst_logp_avg.val:.4f} ({reconst_logp_avg.avg:.4f})\\t'\n",
    "                  'kl {kl_avg.val:.4f} ({kl_avg.avg:.4f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, reconst_logp_avg=reconst_logp_avg, kl_avg=kl_avg,\n",
    "                   loss=loss_avg))\n",
    "\n",
    "    return loss_avg.avg, kl_avg.avg, reconst_logp_avg.avg\n",
    "\n",
    "\n",
    "def validateVAE(val_loader, model, criterion, args):\n",
    "    \"\"\"\n",
    "    iterate through the validate set and output the accuracy\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    loss_avg = AverageMeter()\n",
    "    kl_avg = AverageMeter()\n",
    "    reconst_logp_avg = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, _) in enumerate(val_loader):\n",
    "        if args.cuda:\n",
    "            input = input.cuda()\n",
    "\n",
    "        # compute output\n",
    "        recon_batch, mu, logvar = model(input)\n",
    "        loss, loss_details = criterion(recon_batch, input, mu, logvar)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        loss_avg.update(loss.item(), input.size(0))\n",
    "        kl_avg.update(loss_details['KL'].item(), input.size(0))\n",
    "        reconst_logp_avg.update(loss_details['reconst_logp'].item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'reconst_logp {reconst_logp_avg.val:.4f} ({reconst_logp_avg.avg:.4f})\\t'\n",
    "                  'kl {kl_avg.val:.4f} ({kl_avg.avg:.4f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, reconst_logp_avg=reconst_logp_avg,\n",
    "                   kl_avg=kl_avg, loss=loss_avg))\n",
    "    return loss_avg.avg, kl_avg.avg, reconst_logp_avg.avg\n",
    "\n",
    "def evaluateVAE(test_loader, model, criterion, args):\n",
    "    \"\"\"\n",
    "    iterate through test loader and find out average loss of normal and\n",
    "    abnormal\n",
    "    \"\"\"\n",
    "    avg_abnormal_loss = AverageMeter()\n",
    "    avg_normal_loss = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for i, (input, target) in tqdm(enumerate(test_loader)):\n",
    "       if args.cuda:\n",
    "           input = input.cuda()\n",
    "\n",
    "       # compute output\n",
    "       recon_batch, mu, logvar = model(input)\n",
    "       loss, loss_details = criterion(recon_batch, input, mu, logvar)\n",
    "\n",
    "       # if normal\n",
    "       if target.item() == 1:\n",
    "           avg_normal_loss.update(loss.item(), input.size(0))\n",
    "       else:\n",
    "           avg_abnormal_loss.update(loss.item(), input.size(0))\n",
    "\n",
    "    return avg_normal_loss.avg, avg_abnormal_loss.avg\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
