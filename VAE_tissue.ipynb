{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a VAE model\n",
    "For 2D input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "depth = 64      # initial depth to convolve channels into\n",
    "n_channels = 3  # number of channels (RGB)\n",
    "filt_size = 4   # convolution filter size\n",
    "stride = 2      # stride for conv\n",
    "pad = 1         # padding added for conv\n",
    "\n",
    "class VAE2D(nn.Module):\n",
    "    def __init__(self, img_size, n_latent=300):\n",
    "        \n",
    "        # Model setup\n",
    "        #############\n",
    "        super(VAE2D, self).__init__()\n",
    "        self.n_latent = n_latent\n",
    "        n = math.log2(img_size)\n",
    "        assert n == round(n), 'Image size must be a power of 2'  # restrict image input sizes permitted\n",
    "        assert n >= 3, 'Image size must be at least 8'           # low dimensional data won't work well\n",
    "        n = int(n)\n",
    "\n",
    "        # Encoder - first half of VAE\n",
    "        #############################\n",
    "        self.encoder = nn.Sequential()  \n",
    "        # input: n_channels x img_size x img_size\n",
    "        # ouput: depth x conv_img_size^2\n",
    "        # conv_img_size = (img_size - filt_size + 2 * pad) / stride + 1\n",
    "        self.encoder.add_module('input-conv', nn.Conv2d(n_channels, depth, filt_size, stride, pad,\n",
    "                                                        bias=True))\n",
    "        self.encoder.add_module('input-relu', nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Add conv layer for each power of 2 over 3 (min size)\n",
    "        # Pyramid strategy with batch normalization added\n",
    "        for i in range(n - 3):\n",
    "            # input: depth x conv_img_size^2\n",
    "            # output: o_depth x conv_img_size^2\n",
    "            # i_depth = o_depth of previous layer\n",
    "            i_depth = depth * 2 ** i\n",
    "            o_depth = depth * 2 ** (i + 1)\n",
    "            self.encoder.add_module(f'pyramid_{i_depth}-{o_depth}_conv',\n",
    "                                    nn.Conv2d(i_depth, o_depth, filt_size, stride, pad, bias=True))\n",
    "            self.encoder.add_module(f'pyramid_{o_depth}_batchnorm',\n",
    "                                    nn.BatchNorm2d(o_depth))\n",
    "            self.encoder.add_module(f'pyramid_{o_depth}_relu',\n",
    "                                    nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Latent representation\n",
    "        #######################\n",
    "        # Convolve the encoded image into the latent space, once for mu and once for logvar\n",
    "        max_depth = depth * 2 ** (n - 3)\n",
    "        self.conv_mu = nn.Conv2d(max_depth, n_latent, filt_size)      # return the mean of the latent space \n",
    "        self.conv_logvar = nn.Conv2d(max_depth, n_latent, filt_size)  # return the log variance of the same\n",
    "        \n",
    "        \n",
    "        # Decoder - second half of VAE\n",
    "        ##############################\n",
    "        self.decoder = nn.Sequential()\n",
    "        # input: max_depth x conv_img_size^2 (8 x 8)  TODO double check sizes\n",
    "        # output: n_latent x conv_img_size^2 (8 x 8)\n",
    "        # default stride=1, pad=0 for this layer\n",
    "        self.decoder.add_module('input-conv', nn.ConvTranspose2d(n_latent, max_depth, filt_size, bias=True))\n",
    "        self.decoder.add_module('input-batchnorm', nn.BatchNorm2d(max_depth))\n",
    "        self.decoder.add_module('input-relu', nn.ReLU(inplace=True))\n",
    "    \n",
    "        # Reverse the convolution pyramids used in the encoder\n",
    "        for i in range(n - 3, 0, -1):\n",
    "            i_depth = depth * 2 ** i\n",
    "            o_depth = depth * 2 ** (i - 1)\n",
    "            self.decoder.add_module(f'pyramid_{i_depth}-{o_depth}_conv',\n",
    "                                    nn.ConvTranspose2d(i_depth, o_depth, filt_size, stride, pad, bias=True))\n",
    "            self.decoder.add_module(f'pyramid_{o_depth}_batchnorm',\n",
    "                                    nn.BatchNorm2d(o_depth))\n",
    "            self.decoder.add_module(f'pyramid_{o_depth}_relu', nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Final transposed convolution to return to img_size\n",
    "        # Final activation is tanh instead of relu to allow negative pixel output\n",
    "        self.decoder.add_module('output-conv', nn.ConvTranspose2d(depth, n_channels,\n",
    "                                                                  filt_size, stride, pad, bias=True))\n",
    "        self.decoder.add_module('output-tanh', nn.Tanh())\n",
    "\n",
    "        # Model weights init\n",
    "        ####################\n",
    "        # Randomly initialize the model weights using kaiming method\n",
    "        # Reference: \"Delving deep into rectifiers: Surpassing human-level\n",
    "        # performance on ImageNet classification\" - He, K. et al. (2015)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def encode(self, imgs):\n",
    "        \"\"\"\n",
    "        Encode the images into latent space vectors (mean and log variance representation)\n",
    "        input:  imgs   [batch_size, 3, 256, 256]\n",
    "        output: mu     [batch_size, n_latent, 1, 1]\n",
    "                logvar [batch_size, n_latent, 1, 1]\n",
    "        \"\"\"\n",
    "        output = self.encoder(imgs)\n",
    "        output = output.squeeze(-1).squeeze(-1)\n",
    "        return [self.conv_mu(output), self.conv_logvar(output)]\n",
    "\n",
    "    def generate(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Generates a random latent vector using the trained mean and log variance representation\n",
    "        input:  mu     [batch_size, n_latent, 1, 1]\n",
    "                logvar [batch_size, n_latent, 1, 1]\n",
    "        output: gen    [batch_size, n_latent, 1, 1]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        gen = torch.randn_like(std)\n",
    "        return gen.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, gen):\n",
    "        \"\"\"\n",
    "        Restores an image representation from the generated latent vector\n",
    "        input:  gen      [batch_size, n_latent, 1, 1]\n",
    "        output: gen_imgs [batch_size, 3, 256, 256]\n",
    "        \"\"\"\n",
    "        return self.decoder(gen)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        \"\"\"\n",
    "        Generates reconstituted images from input images based on learned representation\n",
    "        input: imgs     [batch_size, 3, 256, 256]\n",
    "        ouput: gen_imgs [batch_size, 3, 256, 256]\n",
    "               mu       [batch_size, n_latent]\n",
    "               logvar   [batch_size, n_latent]\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(imgs)\n",
    "        gen = self.generate(mu, logvar)\n",
    "        return (self.decode(gen),\n",
    "                mu.squeeze(-1).squeeze(-1),\n",
    "                logvar.squeeze(-1).squeeze(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE2D(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE2D(\n",
       "  (encoder): Sequential(\n",
       "    (input-conv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (input-relu): ReLU(inplace)\n",
       "    (pyramid_64-128_conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_128_batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_128_relu): ReLU(inplace)\n",
       "    (pyramid_128-256_conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_256_batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_256_relu): ReLU(inplace)\n",
       "    (pyramid_256-512_conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_512_batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_512_relu): ReLU(inplace)\n",
       "    (pyramid_512-1024_conv): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_1024_batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_1024_relu): ReLU(inplace)\n",
       "    (pyramid_1024-2048_conv): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_2048_batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_2048_relu): ReLU(inplace)\n",
       "  )\n",
       "  (conv_mu): Conv2d(2048, 300, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv_logvar): Conv2d(2048, 300, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (decoder): Sequential(\n",
       "    (input-conv): ConvTranspose2d(300, 2048, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (input-batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (input-relu): ReLU(inplace)\n",
       "    (pyramid_2048-1024_conv): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_1024_batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_1024_relu): ReLU(inplace)\n",
       "    (pyramid_1024-512_conv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_512_batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_512_relu): ReLU(inplace)\n",
       "    (pyramid_512-256_conv): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_256_batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_256_relu): ReLU(inplace)\n",
       "    (pyramid_256-128_conv): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_128_batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_128_relu): ReLU(inplace)\n",
       "    (pyramid_128-64_conv): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (pyramid_64_batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_64_relu): ReLU(inplace)\n",
       "    (output-conv): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (output-tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$PYTHONBIN train.py --data ${DATA_DIR} --cuda \\\n",
    "    --epochs 40 --lr 1e-4 --batch_size 32 --out_dir ${EXP_DIR}/NV_kl0.01 \\\n",
    "    --image_size 128 --kl_weight 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import argparse\n",
    "from loss import VAELoss\n",
    "from torchvision.utils import make_grid\n",
    "from utilities import trainVAE, validateVAE\n",
    "from model import VAE\n",
    "from dataloader import load_vae_train_datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "# Model\n",
    "desc = 'VAE for detecting anomalies in 2D images'\n",
    "data_path = Path('data/NV_outlier/')\n",
    "img_size = 256\n",
    "\n",
    "# Training\n",
    "epochs = 10\n",
    "lr = 1e-4        # learning rate\n",
    "lr_decay = 0.1\n",
    "schedule = 5     # decrease lr after this many epochs\n",
    "batch_size = 64\n",
    "kl = 0.1         # weight of the kl term\n",
    "\n",
    "# Checkpoints and logging\n",
    "save_path = Path('model/')  # where model and results will be saved\n",
    "load_path = None            # checkpoint to resume from (default None)\n",
    "log_freq = 10               # print status after this many batches\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load checkpoint\n",
    "if args.resume is not None:\n",
    "    checkpoint = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
    "    print(\"checkpoint loaded!\")\n",
    "    print(\"val loss: {}\\tepoch: {}\\t\".format(checkpoint['val_loss'], checkpoint['epoch']))\n",
    "\n",
    "# model\n",
    "model = VAE(args.image_size)\n",
    "if args.resume is not None:\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# criterion\n",
    "criterion = VAELoss(size_average=True, kl_weight=args.kl_weight)\n",
    "if args.cuda is True:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "# load data\n",
    "train_loader, val_loader = load_vae_train_datasets(input_size=args.image_size,\n",
    "                                                   data=args.data,\n",
    "                                                   batch_size=args.batch_size)\n",
    "\n",
    "# load optimizer and scheduler\n",
    "opt = torch.optim.Adam(params=model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n",
    "if args.resume is not None and not args.reset_opt:\n",
    "    opt.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=args.schedule,\n",
    "                                                 gamma=args.lr_decay)\n",
    "\n",
    "# make output dir\n",
    "if os.path.isdir(args.out_dir):\n",
    "    print(\"{} already exists!\".format(args.out_dir))\n",
    "os.mkdir(args.out_dir)\n",
    "\n",
    "# save args\n",
    "args_dict = vars(args)\n",
    "with open(os.path.join(args.out_dir, 'config.txt'), 'w') as f:\n",
    "    for k in args_dict.keys():\n",
    "        f.write(\"{}:{}\\n\".format(k, args_dict[k]))\n",
    "writer = SummaryWriter(log_dir=os.path.join(args.out_dir, 'logs'))\n",
    "\n",
    "# main loop\n",
    "best_loss = np.inf\n",
    "for epoch in range(args.epochs):\n",
    "    # train for one epoch\n",
    "    scheduler.step()\n",
    "    train_loss, train_kl, train_reconst_logp = trainVAE(train_loader, model, criterion, opt, epoch, args)\n",
    "    writer.add_scalar('train_elbo', -train_loss, global_step=epoch + 1)\n",
    "    writer.add_scalar('train_kl', train_kl, global_step=epoch + 1)\n",
    "    writer.add_scalar('train_reconst_logp', train_reconst_logp, global_step=epoch + 1)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_kl, val_reconst_logp = validateVAE(val_loader, model, criterion, args)\n",
    "        writer.add_scalar('val_elbo', -val_loss, global_step=epoch + 1)\n",
    "        writer.add_scalar('val_kl', val_kl, global_step=epoch + 1)\n",
    "        writer.add_scalar('val_reconst_logp', val_reconst_logp, global_step=epoch + 1)\n",
    "\n",
    "    # remember best acc and save checkpoint\n",
    "    if val_loss < best_loss:\n",
    "        print('checkpointed!')\n",
    "        best_loss = val_loss\n",
    "        save_dict = {'epoch': epoch + 1,\n",
    "                     'state_dict': model.state_dict(),\n",
    "                     'val_loss': val_loss,\n",
    "                     'optimizer': opt.state_dict()}\n",
    "        save_path = os.path.join(args.out_dir, 'best_model.pth.tar')\n",
    "        torch.save(save_dict, save_path)\n",
    "    print('curr lowest val loss {}'.format(best_loss))\n",
    "\n",
    "    # visualize reconst and free sample\n",
    "    print(\"plotting imgs...\")\n",
    "    with torch.no_grad():\n",
    "        val_iter = val_loader.__iter__()\n",
    "\n",
    "        # reconstruct 25 imgs\n",
    "        imgs = val_iter._get_batch()[1][0][:25]\n",
    "        if args.cuda:\n",
    "            imgs = imgs.cuda()\n",
    "        imgs_reconst, mu, logvar = model(imgs)\n",
    "\n",
    "        # sample 25 imgs\n",
    "        noises = torch.randn(25, model.nz, 1, 1)\n",
    "        if args.cuda:\n",
    "            noises = noises.cuda()\n",
    "        samples = model.decode(noises)\n",
    "\n",
    "        def write_image(tag, images):\n",
    "            \"\"\"\n",
    "            write the resulting imgs to tensorboard.\n",
    "            :param tag: The tag for tensorboard\n",
    "            :param images: the torch tensor with range (-1, 1). [9, 3, 256, 256]\n",
    "            \"\"\"\n",
    "            # make it from 0 to 255\n",
    "            images = (images + 1) / 2\n",
    "            grid = make_grid(images, nrow=5, padding=20)\n",
    "            writer.add_image(tag, grid.detach(), global_step=epoch + 1)\n",
    "\n",
    "        write_image(\"origin\", imgs)\n",
    "        write_image(\"reconst\", imgs_reconst)\n",
    "        write_image(\"samples\", samples)\n",
    "        print('done')\n",
    "\n",
    "import ipdb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
