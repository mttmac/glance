{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of VAE for anomaly detection of diverse sensor data\n",
    "Trained on non-fault data to learn a normal expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model for 512 time points per cycle with 14 sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from demo import *\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accumulator'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-series size = 512, number of sensors = 14, latent size = 50\n"
     ]
    }
   ],
   "source": [
    "model = VAE1D(size, n_channels, n_latent)\n",
    "model = model.to(device)\n",
    "print(f\"Time-series size = {size}, number of sensors = {n_channels}, \"\n",
    "      f\"latent size = {n_latent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best parameters from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Performance:\n",
      "Validation loss: 14.268\n",
      "Epoch: 210\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([1, 14, 512])\n",
      "Encoded size: torch.Size([1, 1024, 4])\n",
      "Latent size: torch.Size([1, 50, 1])\n",
      "Decoded (output) size: torch.Size([1, 14, 512])\n"
     ]
    }
   ],
   "source": [
    "params = model.demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of random data = 666.951\n"
     ]
    }
   ],
   "source": [
    "X, E, L, D = [param.cpu().detach().numpy() for param in params]\n",
    "MSE = np.power((X - D), 2).sum() / 2\n",
    "print(f\"MSE of random data = {MSE:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.03756665, 0.39188507, 0.679316  , 0.6438357 ],\n",
       "        [0.8571021 , 0.16819857, 0.9926153 , 0.633139  ],\n",
       "        [0.14229044, 0.24964722, 0.15595786, 0.7348866 ],\n",
       "        [0.7716662 , 0.80065954, 0.3989681 , 0.01456177]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, :4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.1584080e-01, 2.1162044e-01, 2.3755118e-01, 4.0655178e-01],\n",
       "        [3.6056536e-01, 1.4748839e-01, 6.9426149e-02, 2.3096055e-04],\n",
       "        [6.4407456e-01, 5.3543776e-01, 5.1022458e-01, 4.0813673e-01],\n",
       "        [2.2680111e-01, 3.4148991e-01, 3.6603764e-01, 6.2843657e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[:, :4, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to load the datasets of sensors readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(f'data/hydraulic/{desc}')\n",
    "train_dl, val_dl, test_dl = load_datasets(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 7 1157\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dl), len(val_dl), len(test_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some random samples from the dataset for examples and generate new versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = get_random_samples(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 = fail\n",
      "1 = norm\n"
     ]
    }
   ],
   "source": [
    "classes = list_target_classes(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = targets.cpu().numpy()\n",
    "targets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normals =  349\n"
     ]
    }
   ],
   "source": [
    "print('normals = ', targets.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failures =  808\n"
     ]
    }
   ],
   "source": [
    "print('failures = ', len(targets) - targets.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = VAE1DLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Generate\n",
    "    data = data.to(device)\n",
    "    gen_data, mu, logvar = model(data)\n",
    "    loss, loss_desc = criterion(gen_data, data, mu, logvar, reduce=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1756,  3.2176, 16.6307,  1.3021,  8.2106,  3.6154,  3.1698,  2.0904,\n",
       "         2.7066, 11.7679], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_err = -loss_desc['logp']\n",
    "gen_err[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5a1080dadbe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgen_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgen_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "sample = data[idx, :, :].cpu().numpy()\n",
    "gen_sample = gen_data[idx, :, :].cpu().numpy()\n",
    "plt.plot(sample[0, :])\n",
    "plt.plot(gen_sample[0, :])\n",
    "print(sample[:5, :5] - gen_sample[:5, :5])\n",
    "print(f\"MSE = {np.power((sample - gen_sample), 2).sum() / 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "sample = data[idx, :, :].cpu().numpy()\n",
    "gen_sample = gen_data[idx, :, :].cpu().numpy()\n",
    "plt.plot(sample[0, :])\n",
    "plt.plot(gen_sample[0, :])\n",
    "print(sample[:5, :5] - gen_sample[:5, :5])\n",
    "print(f\"MSE = {np.power((sample - gen_sample), 2).sum() / 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.zeros(targets.shape)\n",
    "for i, target in enumerate(targets):\n",
    "    err[i] = -loss_desc['logp'][i]\n",
    "    print(f\"Target = {target}, MSE = {err[i]:.3f}\")\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(targets, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the data plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(gen_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's score the success of the recreation and look for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score(test_dl, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_means = pd.DataFrame()\n",
    "for (name, cls), item in scores.items():\n",
    "    test_means.loc[name, cls] = np.array(item).mean()\n",
    "\n",
    "print(\"###################### TEST MEANS #####################\")\n",
    "print(test_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_means = pd.DataFrame()\n",
    "for (name, cls), item in score(val_dl, model, criterion).items():\n",
    "    val_means.loc[name, cls] = np.array(item).mean()\n",
    "\n",
    "print(\"###################### VAL MEANS #####################\")\n",
    "print(val_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_means = pd.DataFrame()\n",
    "for (name, cls), item in score(train_dl, model, criterion).items():\n",
    "    train_means.loc[name, cls] = np.array(item).mean()\n",
    "\n",
    "print(\"###################### TRAIN MEANS #####################\")\n",
    "print(train_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate AUC to judge performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(scores[('error', 'norm')]), len(scores[('error', 'fail')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple definition of the threshold as mean between sets\n",
    "threshold = (np.mean(scores['error', 'fail']) +\n",
    "             np.mean(scores['error', 'norm'])) / 2\n",
    "print('mean threshold:', threshold)\n",
    "# maximum 95% percentile of normal as threshold\n",
    "# threshold = np.percentile(scores['error', 'norm'], 95)\n",
    "# print('95th percentile threshold:', threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scores = scores.copy()\n",
    "t_scores[('error', 'fail')] = (t_scores[('error', 'fail')] > threshold)\n",
    "t_scores[('error', 'norm')] = (t_scores[('error', 'norm')] > threshold)\n",
    "\n",
    "t_score = []\n",
    "t_score.extend(t_scores[('error', 'fail')])\n",
    "t_score.extend(t_scores[('error', 'norm')])\n",
    "t_score = np.array(t_score)\n",
    "\n",
    "y_true = []\n",
    "y_true.extend([True] * len(t_scores[('error', 'fail')]))\n",
    "y_true.extend([False] * len(t_scores[('error', 'norm')]))\n",
    "y_true = np.array(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_corr = t_score == y_true\n",
    "t_corr.sum() / len(t_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores = auc_score(test_dl, t_scores)\n",
    "auc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores['error', 'norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaly = sorted(scores[('error', 'norm')])\n",
    "anomaly = sorted(scores[('error', 'fail')])\n",
    "plt.plot(normaly, label='Normal MSE')\n",
    "plt.plot(anomaly, label='Anomaly MSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(normaly, label='Normal MSE', bins=100, alpha=0.5)\n",
    "plt.hist(anomaly, label='Anomaly MSE', bins=100, alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalx = np.random.rand(len(normaly))\n",
    "anomalx = np.random.rand(len(anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(normaly, normalx, label='Normal MSE (test)', alpha=0.5)\n",
    "plt.scatter(anomaly, anomalx, label='Anomaly MSE (test)', alpha=0.5)\n",
    "plt.plot([threshold, threshold], [0, 1], 'k-')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try fitting a normal distribution to the validation MSE to define a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = score(val_dl, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_norm = sorted(val_scores[('error', 'norm')])\n",
    "print(val_norm[:5])\n",
    "mean = np.mean(val_norm)\n",
    "std = np.std(val_norm)\n",
    "print('mean={:.2f}, stdev={:.2f}'.format(mean, std))\n",
    "plt.plot(val_norm, label= 'Normal MSE (validation)')\n",
    "plt.plot([0, len(val_norm)], [mean, mean], label='Mean')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement PCA to visualize the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents, targets = compute_latent(test_dl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_pca = pca.fit_transform(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_mask = targets == 0\n",
    "norm_mask = targets == 1\n",
    "plt.scatter(lat_pca[fail_mask, 0], lat_pca[fail_mask, 1],\n",
    "            c='orange', label='fault')\n",
    "plt.scatter(lat_pca[norm_mask, 0], lat_pca[norm_mask, 1],\n",
    "            c='skyblue', label='normal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the latent space into kmeans clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_k = kmeans.fit_transform(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lat_pca[:, 0], lat_pca[:, 1], c=kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding the error terms to the latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents, kl, error, targets = compute_latent_and_loss(test_dl, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latents.shape, kl.shape, error.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.hstack([latents, kl[:, None], error[:, None]])\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "lat_pca = pca.fit_transform(features)\n",
    "lat_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lat_pca[fail_mask, 0], lat_pca[fail_mask, 1],\n",
    "            c='orange', label='fault')\n",
    "plt.scatter(lat_pca[norm_mask, 0], lat_pca[norm_mask, 1],\n",
    "            c='skyblue', label='normal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the pca plots for validation and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents, targets = compute_latent(train_dl, model)\n",
    "pca = PCA(n_components=2)\n",
    "lat_pca = pca.fit_transform(latents)\n",
    "plt.scatter(lat_pca[:, 0], lat_pca[:, 1],\n",
    "            c='skyblue', label='normal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents, targets = compute_latent(val_dl, model)\n",
    "pca = PCA(n_components=2)\n",
    "lat_pca = pca.fit_transform(latents)\n",
    "plt.scatter(lat_pca[:, 0], lat_pca[:, 1],\n",
    "            c='skyblue', label='normal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the validation plot with the test plot to understand global structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents, targets = compute_latent(train_dl, model)\n",
    "pca = PCA(n_components=2)\n",
    "train_pca = pca.fit_transform(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents, targets = compute_latent(val_dl, model)\n",
    "val_pca = pca.transform(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents, targets = compute_latent(test_dl, model)\n",
    "test_pca = pca.transform(latents)\n",
    "fail_mask = targets == 0\n",
    "norm_mask = targets == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_pca[fail_mask, 0], test_pca[fail_mask, 1],\n",
    "            c='orange', label=desc + ' fault')\n",
    "plt.scatter(test_pca[norm_mask, 0], test_pca[norm_mask, 1],\n",
    "            c='indigo', label='normal-test')\n",
    "plt.scatter(val_pca[:, 0], val_pca[:, 1],\n",
    "            c='midnightblue', label='normal-val')\n",
    "plt.scatter(train_pca[:, 0], train_pca[:, 1],\n",
    "            c='darkslateblue', label='normal-train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_pca[norm_mask, 0], test_pca[norm_mask, 1],\n",
    "            c='indigo', label='normal-test')\n",
    "plt.scatter(val_pca[:, 0], val_pca[:, 1],\n",
    "            c='midnightblue', label='normal-val')\n",
    "plt.scatter(train_pca[:, 0], train_pca[:, 1],\n",
    "            c='darkslateblue', label='normal-train')\n",
    "plt.scatter(test_pca[fail_mask, 0], test_pca[fail_mask, 1],\n",
    "            c='orange', label=desc + ' fault')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
