{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAE1D import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model and loss function \n",
    "Initially designed for 2D input images, modified for 1D time-series data.\n",
    "Based on this paper: https://arxiv.org/abs/1807.01349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hydraulic system has 14 sensors from which to pull data\n",
    "n_channels = 14\n",
    "# The data has been resized to 512, although it represents 1 min for each cycle\n",
    "size = 512\n",
    "# Latent space is restricted to be about 1/170th of the input dims, similar to 2D case\n",
    "n_latent = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE1D(\n",
       "  (encoder): Sequential(\n",
       "    (input-conv): Conv1d(14, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (input-relu): ReLU(inplace)\n",
       "    (pyramid_64-128_conv): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_128_batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_128_relu): ReLU(inplace)\n",
       "    (pyramid_128-256_conv): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_256_batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_256_relu): ReLU(inplace)\n",
       "    (pyramid_256-512_conv): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_512_batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_512_relu): ReLU(inplace)\n",
       "    (pyramid_512-1024_conv): Conv1d(512, 1024, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_1024_batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_1024_relu): ReLU(inplace)\n",
       "    (pyramid_1024-2048_conv): Conv1d(1024, 2048, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_2048_batchnorm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_2048_relu): ReLU(inplace)\n",
       "    (pyramid_2048-4096_conv): Conv1d(2048, 4096, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_4096_batchnorm): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_4096_relu): ReLU(inplace)\n",
       "  )\n",
       "  (conv_mu): Conv1d(4096, 42, kernel_size=(4,), stride=(1,))\n",
       "  (conv_logvar): Conv1d(4096, 42, kernel_size=(4,), stride=(1,))\n",
       "  (decoder): Sequential(\n",
       "    (input-conv): ConvTranspose1d(42, 4096, kernel_size=(4,), stride=(1,))\n",
       "    (input-batchnorm): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (input-relu): ReLU(inplace)\n",
       "    (pyramid_4096-2048_conv): ConvTranspose1d(4096, 2048, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_2048_batchnorm): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_2048_relu): ReLU(inplace)\n",
       "    (pyramid_2048-1024_conv): ConvTranspose1d(2048, 1024, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_1024_batchnorm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_1024_relu): ReLU(inplace)\n",
       "    (pyramid_1024-512_conv): ConvTranspose1d(1024, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_512_batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_512_relu): ReLU(inplace)\n",
       "    (pyramid_512-256_conv): ConvTranspose1d(512, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_256_batchnorm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_256_relu): ReLU(inplace)\n",
       "    (pyramid_256-128_conv): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_128_batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_128_relu): ReLU(inplace)\n",
       "    (pyramid_128-64_conv): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (pyramid_64_batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pyramid_64_relu): ReLU(inplace)\n",
       "    (output-conv): ConvTranspose1d(64, 14, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE1D(size, n_channels, n_latent)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1  # equal KL term to start\n",
    "criterion = VAE1DLoss(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load hydraulic test data\n",
    "From this dataset: https://archive.ics.uci.edu/ml/datasets/Condition+monitoring+of+hydraulic+systems#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data/hydraulic')\n",
    "train_dl, val_dl, test_dl = load_datasets(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 10 720\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dl), len(val_dl), len(test_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder models/190128-hydraulic already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('models/190128-hydraulic')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = 'hydraulic'\n",
    "log_freq = 5 # batches\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "lr = 1e-1                # learning rate\n",
    "lr_decay = 0.1           # lr decay factor\n",
    "schedule = [25, 50, 75]  # decrease lr at these epochs\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Checkpoint to resume from (default None)\n",
    "load_path = None\n",
    "\n",
    "# Checkpoint save location\n",
    "save_path = Path(f\"models/{date.today().strftime('%y%m%d')}-{desc}/\")\n",
    "if save_path.is_dir():\n",
    "    print(f\"Folder {save_path} already exists\")\n",
    "else:\n",
    "    os.mkdir(save_path)\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint if any\n",
    "if load_path is not None:\n",
    "    checkpoint = torch.load(load_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print(\"Checkpoint loaded\")\n",
    "    print(f\"Validation loss: {checkpoint['val_loss']}\")\n",
    "    print(f\"Epoch: {checkpoint['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, schedule, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add in logging to Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VAE1D(dl):\n",
    "    \"\"\"\n",
    "    Execute the training loop\n",
    "    \"\"\"\n",
    "    loss_tracker = AvgTracker()\n",
    "    kl_tracker = AvgTracker()\n",
    "    logp_tracker = AvgTracker()\n",
    "    timer = StopWatch()\n",
    "    \n",
    "    for i, (X, _) in enumerate(tqdm(dl)):\n",
    "        \n",
    "        X = X.to(device)\n",
    "        timer.lap()  # load time\n",
    "        \n",
    "        # Generate transient and compute loss\n",
    "        X_hat, mu, logvar = model(X)\n",
    "        loss, loss_desc = criterion(X_hat, X, mu, logvar)\n",
    "        timer.lap()  # gen time\n",
    "        \n",
    "        loss_tracker.update(loss.item())\n",
    "        kl_tracker.update(loss_desc['KL'].item())\n",
    "        logp_tracker.update(loss_desc['logp'].item())\n",
    "        \n",
    "        if model.training:\n",
    "            # Update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            timer.lap()  # backprop time\n",
    "        \n",
    "        if (i + 1) % log_freq == 0:\n",
    "            # Print progress\n",
    "            print(f'Epoch: {epoch + 1} ({i + 1}/{len(dl)})')\n",
    "            print(f'\\tData load time: {timer.elapsed[0]:.3f} sec')\n",
    "            print(f'\\tGeneration time: {timer.elapsed[1]:.3f} sec')\n",
    "            if model.training:\n",
    "                print(f'\\tBackprop time: {timer.elapsed[2]:.3f} sec')\n",
    "            print(f'\\tLog probability: {logp_tracker.val:.4f} '\n",
    "                  f'(avg {logp_tracker.avg:.4f})')\n",
    "            print(f'\\tKL: {kl_tracker.val:.4f} (avg {kl_tracker.avg:.4f})')\n",
    "            print(f'\\tLoss: {loss_tracker.val:.4f} (avg {loss_tracker.avg:.4f})')\n",
    "\n",
    "    return loss_tracker.avg, kl_tracker.avg, logp_tracker.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 5/38 [00:02<00:13,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (5/38)\n",
      "\tData load time: 0.151 sec\n",
      "\tGeneration time: 0.199 sec\n",
      "\tBackprop time: 0.388 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 10/38 [00:03<00:10,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (10/38)\n",
      "\tData load time: 0.151 sec\n",
      "\tGeneration time: 0.199 sec\n",
      "\tBackprop time: 0.388 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 15/38 [00:05<00:08,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (15/38)\n",
      "\tData load time: 0.151 sec\n",
      "\tGeneration time: 0.199 sec\n",
      "\tBackprop time: 0.388 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 20/38 [00:07<00:06,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (20/38)\n",
      "\tData load time: 0.151 sec\n",
      "\tGeneration time: 0.199 sec\n",
      "\tBackprop time: 0.388 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 25/38 [00:09<00:05,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (25/38)\n",
      "\tData load time: 0.151 sec\n",
      "\tGeneration time: 0.199 sec\n",
      "\tBackprop time: 0.388 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 30/38 [00:11<00:03,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (30/38)\n",
      "\tData load time: 0.151 sec\n",
      "\tGeneration time: 0.199 sec\n",
      "\tBackprop time: 0.388 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 35/38 [00:13<00:01,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (35/38)\n",
      "\tData load time: 0.151 sec\n",
      "\tGeneration time: 0.199 sec\n",
      "\tBackprop time: 0.388 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:14<00:00,  2.87it/s]\n",
      " 60%|██████    | 6/10 [00:01<00:00,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (5/10)\n",
      "\tData load time: 0.138 sec\n",
      "\tGeneration time: 0.148 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.01it/s]\n",
      "  0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (10/10)\n",
      "\tData load time: 0.138 sec\n",
      "\tGeneration time: 0.148 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n",
      "Lowest validation loss: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 5/38 [00:01<00:12,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (5/38)\n",
      "\tData load time: 0.132 sec\n",
      "\tGeneration time: 0.144 sec\n",
      "\tBackprop time: 0.303 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 10/38 [00:03<00:10,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (10/38)\n",
      "\tData load time: 0.132 sec\n",
      "\tGeneration time: 0.144 sec\n",
      "\tBackprop time: 0.303 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 15/38 [00:05<00:08,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (15/38)\n",
      "\tData load time: 0.132 sec\n",
      "\tGeneration time: 0.144 sec\n",
      "\tBackprop time: 0.303 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 20/38 [00:07<00:06,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (20/38)\n",
      "\tData load time: 0.132 sec\n",
      "\tGeneration time: 0.144 sec\n",
      "\tBackprop time: 0.303 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 25/38 [00:09<00:05,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (25/38)\n",
      "\tData load time: 0.132 sec\n",
      "\tGeneration time: 0.144 sec\n",
      "\tBackprop time: 0.303 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 30/38 [00:11<00:03,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (30/38)\n",
      "\tData load time: 0.132 sec\n",
      "\tGeneration time: 0.144 sec\n",
      "\tBackprop time: 0.303 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 35/38 [00:13<00:01,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (35/38)\n",
      "\tData load time: 0.132 sec\n",
      "\tGeneration time: 0.144 sec\n",
      "\tBackprop time: 0.303 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:14<00:00,  2.86it/s]\n",
      " 60%|██████    | 6/10 [00:01<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (5/10)\n",
      "\tData load time: 0.169 sec\n",
      "\tGeneration time: 0.178 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.88it/s]\n",
      "  0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (10/10)\n",
      "\tData load time: 0.169 sec\n",
      "\tGeneration time: 0.178 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n",
      "Lowest validation loss: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 5/38 [00:01<00:12,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 (5/38)\n",
      "\tData load time: 0.152 sec\n",
      "\tGeneration time: 0.161 sec\n",
      "\tBackprop time: 0.322 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 10/38 [00:03<00:10,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 (10/38)\n",
      "\tData load time: 0.152 sec\n",
      "\tGeneration time: 0.161 sec\n",
      "\tBackprop time: 0.322 sec\n",
      "\tLog probability: nan (avg nan)\n",
      "\tKL: nan (avg nan)\n",
      "\tLoss: nan (avg nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 12/38 [00:04<00:10,  2.58it/s]Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-17b6afd2dca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_logp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_VAE1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-325bfce40a86>\u001b[0m in \u001b[0;36mtrain_VAE1D\u001b[0;34m(dl)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# gen time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mkl_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlogp_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    train_loss, train_kl, train_logp = train_VAE1D(train_dl)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_kl, val_logp = train_VAE1D(val_dl)\n",
    "\n",
    "    # Report training progress to user\n",
    "    if val_loss < best_loss:\n",
    "        print('Saving checkpoint..')\n",
    "        best_loss = val_loss\n",
    "        save_dict = {'epoch': epoch + 1,\n",
    "                     'state_dict': model.state_dict(),\n",
    "                     'val_loss': val_loss,\n",
    "                     'optimizer': optimizer.state_dict()}\n",
    "        path = save_path / 'best_model.pth.tar'\n",
    "        torch.save(save_dict, path)\n",
    "    print(f'Lowest validation loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in test_dl:\n",
    "    break\n",
    "X = X.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat, mu, logvar = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: ADD VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO include in epoch loop?\n",
    "# TODO look into what the scheduler is for\n",
    "# visualize reconst and free sample\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_iter = iter(val_dl)\n",
    "\n",
    "    # Generate 25 images\n",
    "    imgs = val_iter._get_batch()[1][0][:25]\n",
    "    imgs = imgs.to(device)\n",
    "    gen_imgs, mu, logvar = model(imgs)\n",
    "    \n",
    "    # Scale images back to 0-1\n",
    "    imgs = (imgs + 1) / 2\n",
    "    grid = make_grid(imgs, nrow=5, padding=20)\n",
    "    gen_imgs = (gen_imgs + 1) / 2\n",
    "    gen_grid = make_grid(gen_imgs, nrow=5, padding=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "grid = grid.cpu()\n",
    "gen_grid = gen_grid.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.imshow(grid.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.imshow(gen_grid.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mu = mu.cpu()\n",
    "std = (0.5 * logvar).exp().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in np.random.choice(range(mu.shape[0]), 5):\n",
    "    plt.figure()\n",
    "    mu_eg = mu[i, :].squeeze(-1).squeeze(-1)\n",
    "    plt.plot(mu_eg.numpy())\n",
    "    plt.figure()\n",
    "    std_eg = std[i, :].squeeze(-1).squeeze(-1)\n",
    "    plt.plot(std_eg.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    # Generate some random images\n",
    "    noises = torch.randn(25, model.n_latent, 1, 1)\n",
    "    noises = noises.to(device)\n",
    "    samples = model.decode(noises)\n",
    "    \n",
    "    samples = (samples + 1) / 2\n",
    "    sample_grid = make_grid(samples, nrow=5, padding=20).cpu()\n",
    "    \n",
    "    plt.imshow(sample_grid.permute(1, 2, 0))  # easy way to swapaxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "!! checkpoints seems like the better way to do this !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = save_path / 'full_model.p'\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(model.cpu(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = model.to(device)\n",
    "len(test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def testVAE2D(test_dl):\n",
    "    abnormal_loss_tracker = AvgTracker()\n",
    "    normal_loss_tracker = AvgTracker()\n",
    "\n",
    "    model.eval()\n",
    "    for i, (X, y) in tqdm(enumerate(test_dl)):\n",
    "\n",
    "        X = X.to(device)\n",
    "        X_hat, mu, logvar = model(X)\n",
    "        loss, loss_desc = criterion(X_hat, X, mu, logvar)\n",
    "\n",
    "        # Normal\n",
    "        if target.item() == 1:\n",
    "           normal_loss_tracker.update(loss.item())\n",
    "        # Abnormal\n",
    "        else:\n",
    "           abnormal_loss_tracker.update(loss.item())\n",
    "\n",
    "    return normal_loss_tracker.avg, abnormal_loss_tracker.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Measure a difference score using the model and use it for outlier detection\n",
    "\"\"\"\n",
    "\n",
    "result_path = save_path / 'result.csv'\n",
    "\n",
    "############################# ANOMALY SCORE DEF ##########################\n",
    "def score(model, img, L=5):\n",
    "    \"\"\"\n",
    "    The vae score for a single image, which is basically the loss\n",
    "    input: image: [1, 3, 256, 256]\n",
    "    output: (loss, KL, gen_err)\n",
    "    \"\"\"\n",
    "    image_batch = image.expand(L,\n",
    "                               image.size(1),\n",
    "                               image.size(2),\n",
    "                               image.size(3))\n",
    "    reconst_batch, mu, logvar = vae.forward(image_batch)\n",
    "    vae_loss, loss_details = criterion(reconst_batch, image_batch, mu, logvar)\n",
    "    return vae_loss, loss_details['KL'], -loss_details['reconst_logp']\n",
    "\n",
    "def _log_mean_exp(x, dim):\n",
    "    \"\"\"\n",
    "    A numerical stable version of log(mean(exp(x)))\n",
    "    :param x: The input\n",
    "    :param dim: The dimension along which to take mean with\n",
    "    \"\"\"\n",
    "    # m [dim1, 1]\n",
    "    m, _ = torch.max(x, dim=dim, keepdim=True)\n",
    "\n",
    "    # x0 [dm1, dim2]\n",
    "    x0 = x - m\n",
    "\n",
    "    # m [dim1]\n",
    "    m = m.squeeze(dim)\n",
    "\n",
    "    return m + torch.log(torch.mean(torch.exp(x0),\n",
    "                                    dim=dim))\n",
    "\n",
    "def get_iwae_score(vae, image, L=5):\n",
    "    \"\"\"\n",
    "    The vae score for a single image, which is basically the loss\n",
    "    :param image: [1, 3, 256, 256]\n",
    "    :return scocre: (iwae score, iwae KL, iwae reconst).\n",
    "    \"\"\"\n",
    "    # [L, 3, 256, 256]\n",
    "    image_batch = image.expand(L,\n",
    "                               image.size(1),\n",
    "                               image.size(2),\n",
    "                               image.size(3))\n",
    "\n",
    "    # [L, z_dim, 1, 1]\n",
    "    mu, logvar = vae.encode(image_batch)\n",
    "    eps = torch.randn_like(mu)\n",
    "    z = mu + eps * torch.exp(0.5 * logvar)\n",
    "    kl_weight = criterion.kl_weight\n",
    "    # [L, 3, 256, 256]\n",
    "    reconst = vae.decode(z)\n",
    "    # [L]\n",
    "    log_p_x_z = -torch.sum((reconst - image_batch).pow(2).reshape(L, -1),\n",
    "                          dim=1)\n",
    "\n",
    "    # [L]\n",
    "    log_p_z = -torch.sum(z.pow(2).reshape(L, -1), dim=1)\n",
    "\n",
    "    # [L]\n",
    "    log_q_z = -torch.sum(eps.pow(2).reshape(L, -1), dim=1)\n",
    "\n",
    "    iwae_score = -_log_mean_exp(log_p_x_z + (log_p_z - log_q_z)*kl_weight, dim=0)\n",
    "    iwae_KL_score = -_log_mean_exp(log_p_z - log_q_z, dim=0)\n",
    "    iwae_reconst_score = -_log_mean_exp(log_p_x_z, dim=0)\n",
    "\n",
    "    return iwae_score, iwae_KL_score, iwae_reconst_score\n",
    "\n",
    "############################# END OF ANOMALY SCORE ###########################\n",
    "\n",
    "# Define the number of samples of each score\n",
    "def compute_all_scores(vae, image):\n",
    "    \"\"\"\n",
    "    Given an image compute all anomaly score\n",
    "    return (reconst_score, vae_score, iwae_score)\n",
    "    \"\"\"\n",
    "    vae_loss, KL, reconst_err = get_vae_score(vae, image=image, L=15)\n",
    "    iwae_loss, iwae_KL, iwae_reconst = get_iwae_score(vae, image, L=15)\n",
    "    result = {'reconst_score': reconst_err.item(),\n",
    "              'KL_score': KL.item(),\n",
    "              'vae_score': vae_loss.item(),\n",
    "              'iwae_score': iwae_loss.item(),\n",
    "              'iwae_KL_score': iwae_KL.item(),\n",
    "              'iwae_reconst_score': iwae_reconst.item()}\n",
    "    return result\n",
    "\n",
    "\n",
    "# MAIN LOOP\n",
    "score_names = ['reconst_score', 'KL_score', 'vae_score',\n",
    "               'iwae_reconst_score', 'iwae_KL_score', 'iwae_score']\n",
    "classes = test_loader.dataset.classes\n",
    "scores = {(score_name, cls): [] for (score_name, cls) in product(score_names,\n",
    "                                                                 classes)}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, (image, target) in tqdm(enumerate(test_loader)):\n",
    "        cls = classes[target.item()]\n",
    "        if args.cuda:\n",
    "            image = image.cuda()\n",
    "\n",
    "        score = compute_all_scores(vae=model, image=image)\n",
    "        for name in score_names:\n",
    "            scores[(name, cls)].append(score[name])\n",
    "\n",
    "# display the mean of scores\n",
    "means = np.zeros([len(score_names), len(classes)])\n",
    "for (name, cls) in product(score_names, classes):\n",
    "    means[score_names.index(name), classes.index(cls)] = sum(scores[(name, cls)]) / len(scores[(name, cls)])\n",
    "df_mean = pd.DataFrame(means, index=score_names, columns=classes)\n",
    "print(\"###################### MEANS #####################\")\n",
    "print(df_mean)\n",
    "\n",
    "\n",
    "classes.remove('NV')\n",
    "auc_result = np.zeros([len(score_names), len(classes) + 1])\n",
    "# get auc roc for each class\n",
    "for (name, cls) in product(score_names, classes):\n",
    "    normal_scores = scores[(name, 'NV')]\n",
    "    abnormal_scores = scores[(name, cls)]\n",
    "    y_true = [0]*len(normal_scores) + [1]*len(abnormal_scores)\n",
    "    y_score = normal_scores + abnormal_scores\n",
    "    auc_result[score_names.index(name), classes.index(cls)] = roc_auc_score(y_true, y_score)\n",
    "\n",
    "# add auc roc against all diseases\n",
    "for name in score_names:\n",
    "    normal_scores = scores[(name, 'NV')]\n",
    "    abnormal_scores = np.concatenate([scores[(name, cls)]for cls in classes]).tolist()\n",
    "    y_true = [0]*len(normal_scores) + [1]*len(abnormal_scores)\n",
    "    y_score = normal_scores + abnormal_scores\n",
    "    auc_result[score_names.index(name), -1] = roc_auc_score(y_true, y_score)\n",
    "\n",
    "df = pd.DataFrame(auc_result, index=score_names, columns=classes + ['ALL'])\n",
    "# display\n",
    "print(\"###################### AUC ROC #####################\")\n",
    "print(df)\n",
    "print(\"####################################################\")\n",
    "df.to_csv(args.out_csv)\n",
    "\n",
    "# fit a gamma distribution\n",
    "_, val_loader = load_vae_train_datasets(args.image_size, args.data, 32)\n",
    "model.eval()\n",
    "all_reconst_err = []\n",
    "num_val = len(val_loader.dataset)\n",
    "with torch.no_grad():\n",
    "    for img, _ in tqdm(val_loader):\n",
    "        if args.cuda:\n",
    "            img = img.cuda()\n",
    "\n",
    "        # compute output\n",
    "        recon_batch, mu, logvar = model(img)\n",
    "        loss, loss_details = criterion.forward_without_reduce(recon_batch, img, mu, logvar)\n",
    "        reconst_err = -loss_details['reconst_logp']\n",
    "        all_reconst_err += reconst_err.tolist()\n",
    "\n",
    "fit_alpha, fit_loc, fit_beta=stats.gamma.fit(all_reconst_err)\n",
    "\n",
    "# using gamma for outlier detection\n",
    "# get auc roc for each class\n",
    "LARGE_NUMBER = 1e30\n",
    "\n",
    "def get_gamma_score(scores):\n",
    "    result = -stats.gamma.logpdf(scores, fit_alpha, fit_loc, fit_beta)\n",
    "    # replace inf in result with largest number\n",
    "    result[result == np.inf] = LARGE_NUMBER\n",
    "    return result\n",
    "\n",
    "auc_gamma_result = np.zeros([1, len(classes)+1])\n",
    "name = 'reconst_score'\n",
    "for cls in classes:\n",
    "    normal_scores = get_gamma_score(scores[(name, 'NV')]).tolist()\n",
    "    abnormal_scores = get_gamma_score(scores[(name, cls)]).tolist()\n",
    "    y_true = [0]*len(normal_scores) + [1]*len(abnormal_scores)\n",
    "    y_score = normal_scores + abnormal_scores\n",
    "    auc_gamma_result[0, classes.index(cls)] = roc_auc_score(y_true, y_score)\n",
    "\n",
    "# for all class\n",
    "normal_scores = get_gamma_score(scores[(name, 'NV')]).tolist()\n",
    "abnormal_scores = np.concatenate([get_gamma_score(scores[(name, cls)]) for cls in classes]).tolist()\n",
    "y_true = [0]*len(normal_scores) + [1]*len(abnormal_scores)\n",
    "y_score = normal_scores + abnormal_scores\n",
    "auc_gamma_result[0, -1] = roc_auc_score(y_true, y_score)\n",
    "df = pd.DataFrame(auc_gamma_result, index=['gamma score'], columns=classes + ['ALL'])\n",
    "\n",
    "# display\n",
    "print(\"###################### AUC ROC GAMMA #####################\")\n",
    "print(df)\n",
    "print(\"##########################################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
